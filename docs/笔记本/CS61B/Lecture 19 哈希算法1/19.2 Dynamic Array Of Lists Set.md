我们不再给每个部分分配相同的墙面空间，而是动态地增加每个部分的大小。
最简单的解决方法就是用Linked Lists：

- 如果箱子中没有元素，我们仍然会有一个空间的浪费单位。

- 但是相比BobaCounterSet还是节省了很多不必要的内存的。

---


我们假设整个墙面上有N个元素，总共有M个箱子用于存放这些元素。
如果我们假设这N个元素被均匀的分布在者M个箱子中，那么每个箱子中都有$N/M$个元素。
如果M是常数的话，那么runtime就是$Θ(N)$。

- 解决办法：让M随着N的增加而增加，这样的话每个箱子中能平均包含固定的元素。

- 需要一个方法来将N个元素分类进M个箱子中，对于任意的M都适用，通过每个数的最后一个数来分类仅仅适用于M=10的。要是M≠10那我们该怎么办？

![](附件/Pasted%20image%2020250918212926.png)

---


我们可以把这个问题推广到模运算上来，下面我们就来进行这个操作：

- 适用于任意M（不仅仅是M=10的情况）

- 随机生成均匀分布的数，对它取模以后，落在每个桶中的概率也会比较均匀。
    - example：假设我们有随机整数0...999，用mod 10，大概每个桶(0~9)里面都会有100个数。

- 如果两个模数互质（比如M1=10，M2=3)，那么它们取模的结果之间不会有相关性，可以认为是独立的。
    - example：x mod 10得到个位数，x mod 3得到余数，如果10和3互质，那么知道x mod 10的结果几乎无法推断出x mod 3的结果。

- 如果把模数乘以一个整数(比如M从10变为20)，那么原来的每个箱子都会被一分为二，而这个划分不会影响均匀性。
    - example：用mod 10，那么箱子是0~9。用mod 20，那么箱子是0~19。相当于把每个旧的箱子拆成两个，比如原来mod 10 =3 的数，现在会落到3和13两个箱子中，这样不会破坏随机分布的均匀性。

- 当然还有别的压缩办法，比如只取一个数的最后几位数字，但是这些方法通常不如取模自然。

## Summary：
- 哈希函数通常会先生成一个“大整数”（比如字符串算出来的hashCode）。

- 但我们需要把它压缩到 [0, M-1] 范围里面，才能作为数组的下标。

- 最自然，效果最好的办法就是取模

- 取模有均匀性、独立性等好处，比别的办法（比如只看数字位数）更合理。

---

为了保证contains操作（就是判断一个数在不在这个墙面中）保持常数时间，我们需要让$N/M$保持在某个常数 $k$ 以下，下面有两种操作方式：

1. 当最大的那个箱子中所装有的元素超过k的时候就继续添加新的箱子（这个操作可能会导致产生很多空的箱子出来）。
2. 当所有箱子的平均装有元素超过k的时候再添加新的元素。
![](附件/Pasted%20image%2020250918215739.png)

由于第一种操作方式又慢有你繁琐，还会产生很多空箱子，因此我们采用第二种做法
那么问题又来了，我们每一次又该添加多少箱子呢？

- 当我们增加M的个数时，我们必须将每个数字重新分配给一个新的盒子，这将在添加操作中花费$Θ(N)$时间。
- 我们的目标是让运行时间保持在 $Θ(1)$ 的摊还复杂度。而我们已经在 ArrayList 里见过：**只要我们很少做 $Θ(N)$ 级别的操作，就能保证这一点**。这可能不好理解，下面来深度解析一下：
  - **一、什么是摊还运行时间**
   这是最重要的概念，摊还的意思就是把一个大操作的代价平均分摊到很多个小操作当中。
   
     **举个例子：你有一个钱包和一个存钱罐**
     
     - 你每次从钱包中取1块钱买东西。
     - 但是你的钱包空了以后，你要花很多时间从零钱罐里拿出100块钱放进钱包里（这是个大操作）。**那么虽然拿零钱花了很久**，但是你后面可以连续快速地买100次东西。
     于是：
     - 每次买东西的平均代价就是：大操作的时间➗100+每次取1块钱的时间
     - 这就是摊还时间 。

 - **二、举个程序里的例子（ArrayList扩容）**
     想象你用一个数组装数据，但你不知道会有多少个元素，于是你用这种策略：
   
     - 一开始数组长度是1
     - 每次装不下了，就开一个两倍大的新数组，把旧的复制过去，插如的过程像这样：
     
        📌 假设规则
        
        - 初始：容量 `capacity = 1`，已有元素 `size = 0`。
        - 插入时如果 `size == capacity`，就先扩容（容量翻倍），再插入。
        - 扩容时复制的元素数 = 扩容前的 `size`（即当前已有元素数）。
        - 操作代价 = 复制次数 + 1（插入本身）。
        
![](附件/Pasted%20image%2020250918224655.png)

可以看到，有些操作（扩容）确实比较贵，比如 O(5)，但是它不是每次都发生，
而是很久发生一次。（不知道为什么每次扩容的时候GPT都没把扩容花费的一次代价算进去，不过算不算进去结果都一样）

📊 结果呢？

如果你插入了 N 个元素，总共复制的次数大概是： 

- $1 + 2 + 4 + 8 + ... + N/2 = N - 1$
- 除了插入的次数是2的倍数时需要执行$n/2+1$次以外，其余情况都只执行一次

所以：

- $总耗时 ≈ N 次普通插入 × O(1) + 总共复制 ≈ O(N)$
- $平均下来：每次插入 ≈ O(N) / N = O(1)$
- $这就是摊还 O(1)：虽然有的操作是 O(N)，但整体平均下来，每次是 O(1)。$

---

还有一种方法就是每次扩容都增加常数k个箱子，这种情况会导致平均每次插入所耗费的时间为O(N)，下面我们来分析一下为什么是这样：

🧮 第一步：扩容次数是多少？

每次扩容能多装 k 个元素：

- 第一次扩容后，插入第 1～k 个元素，容量 = k
- 第二次扩容后，插入第 k+1～2k 个元素，容量 = 2k
- 第三次扩容后，插入第 2k+1～3k 个元素，容量 = 3k
- ...

✅ 总扩容次数约为 $n/k$ 

---

🧮 第二步：每次扩容时，复制了多少旧元素？

- 第 1 次扩容：不用复制，旧数组是空的 → 复制 0 次
- 第 2 次扩容：要把前 k个复制过去 → 复制 k次
- 第 3 次扩容：要复制前 2k个 → 复制 2k次
- ...
- 第 m次扩容：要复制前 (m−1) *k个元素
---

🧮 第三步：总复制次数是多少？

把所有扩容时的复制量加起来：

- 总复制次数 $= 0 + k + 2k + 3k + ... + (m - 1)k$
- 提取出常数 k：原式= $k × (0 + 1 + 2 + 3 + ... + (m - 1))= k × [(m - 1) × m / 2]$
- 扩容次数 $m≈n/k$
- 因此这是一个 数量级为 $O(n^2)$的函数！均摊给每一次之后即为$O(n)$.所以我们不采用这种方式
